---
wrapper_template: "kubernetes/docs/base_docs.html"
markdown_includes:
  nav: "kubernetes/docs/shared/_side-navigation.md"
context:
  title: "kubernetes-master charm"
  description: Kubernetes-master Charm reference
keywords: kubernetes-master, charm, config
tags: [reference]
sidebar: k8smain-sidebar
permalink: charm-kubernetes-master.html
layout: [base, ubuntu-com]
toc: False
---

The `kubernetes-master` charm xxx xxx xxxxxx xx x xxxx xxxxx xxx xxxx

## Charm configuration options

<!-- CONFIG STARTS -->
<!--AUTOGENERATED CONFIG TEXT - DO NOT EDIT -->
### addons-registry

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Specify the docker registry to use when applying addons.

DEPRECATED in 1.15: Use the broader 'image-registry' config option instead. If both
options are set, 'addons-registry' will be used to configure the cdk-addons snap until
v1.17 is released. After that, the 'addons-registry' option will have no effect.

---

### allow-privileged

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: auto

Allow kube-apiserver to run in privileged mode. Supported values are
"true", "false", and "auto". If "true", kube-apiserver will run in
privileged mode by default. If "false", kube-apiserver will never run in
privileged mode. If "auto", kube-apiserver will not run in privileged
mode by default, but will switch to privileged mode if gpu hardware is
detected on a worker node.

---

### api-extra-args

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space separated list of flags and key=value pairs that will be passed as arguments to
kube-apiserver. For example a value like this:
  runtime-config=batch/v2alpha1=true profiling=true
will result in kube-apiserver being run with the following options:
  --runtime-config=batch/v2alpha1=true --profiling=true

---

### audit-policy

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
# Don't log read-only requests from the apiserver
- level: None
  users: ["system:apiserver"]
  verbs: ["get", "list", "watch"]
# Don't log kube-proxy watches
- level: None
  users: ["system:kube-proxy"]
  verbs: ["watch"]
  resources:
  - resources: ["endpoints", "services"]
# Don't log nodes getting their own status
- level: None
  userGroups: ["system:nodes"]
  verbs: ["get"]
  resources:
  - resources: ["nodes"]
# Don't log kube-controller-manager and kube-scheduler getting endpoints
- level: None
  users: ["system:unsecured"]
  namespaces: ["kube-system"]
  verbs: ["get"]
  resources:
  - resources: ["endpoints"]
# Log everything else at the Request level.
- level: Request
  omitStages:
  - RequestReceived


Audit policy passed to kube-apiserver via --audit-policy-file.
For more info, please refer to the upstream documentation at
https://kubernetes.io/docs/tasks/debug-application-cluster/audit/

---

### audit-webhook-config

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Audit webhook config passed to kube-apiserver via --audit-webhook-config-file.
For more info, please refer to the upstream documentation at
https://kubernetes.io/docs/tasks/debug-application-cluster/audit/

---

### authorization-mode

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: AlwaysAllow

Comma separated authorization modes. Allowed values are
"RBAC", "Node", "Webhook", "ABAC", "AlwaysDeny" and "AlwaysAllow".

---

### channel

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: 1.16/stable

Snap channel to install Kubernetes master services from

---

### client_password

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Password to be used for admin user (leave empty for random password).

---

### controller-manager-extra-args

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space separated list of flags and key=value pairs that will be passed as arguments to
kube-controller-manager. For example a value like this:
  runtime-config=batch/v2alpha1=true profiling=true
will result in kube-controller-manager being run with the following options:
  --runtime-config=batch/v2alpha1=true --profiling=true

---

### dashboard-auth

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: auto

Method of authentication for the Kubernetes dashboard. Allowed values are "auto",
"basic", and "token". If set to "auto", basic auth is used unless Keystone is
related to kubernetes-master, in which case token auth is used.

---

### default-storage

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: auto

The storage class to make the default storage class. Allowed values are "auto",
"none", "ceph-xfs", "ceph-ext4". Note: Only works in Kubernetes >= 1.10

---

### dns-provider

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: auto

DNS provider addon to use. Can be "auto", "core-dns", "kube-dns", or
"none".

CoreDNS is only supported on Kubernetes 1.14+.

When set to "auto", the behavior is as follows:
- New deployments of Kubernetes 1.14+ will use CoreDNS
- New deployments of Kubernetes 1.13 or older will use KubeDNS
- Upgraded deployments will continue to use whichever provider was
previously used.

---

### dns_domain

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: cluster.local

The local domain for cluster dns
---

### enable-dashboard-addons

**Type**: boolean &nbsp;&nbsp;&nbsp; **Default**: True

Deploy the Kubernetes Dashboard and Heapster addons
---

### enable-keystone-authorization

**Type**: boolean &nbsp;&nbsp;&nbsp; **Default**: False

If true and the Keystone charm is related, users will authorize against
the Keystone server. Note that if related, users will always authenticate
against Keystone.

---

### enable-metrics

**Type**: boolean &nbsp;&nbsp;&nbsp; **Default**: True

If true the metrics server for Kubernetes will be deployed onto the cluster.

---

### enable-nvidia-plugin

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: auto

Load the nvidia device plugin daemonset. Supported values are
"auto" and "false". When "auto", the daemonset will be loaded
only if GPUs are detected. When "false" the nvidia device plugin
will not be loaded.

---

### extra_packages

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space separated list of extra deb packages to install.

---

### extra_sans

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space-separated list of extra SAN entries to add to the x509 certificate
created for the master nodes.

---

### ha-cluster-dns

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

DNS entry to use with the HA Cluster subordinate charm.
Mutually exclusive with ha-cluster-vip.

---

### ha-cluster-vip

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Virtual IP for the charm to use with the HA Cluster subordinate charm
Mutually exclusive with ha-cluster-dns. Multiple virtual IPs are
separated by spaces.

---

### image-registry

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: rocks.canonical.com:443/cdk

Container image registry to use for CDK. This includes addons like the Kubernetes dashboard,
metrics server, ingress, and dns along with non-addon images including the pause
container and default backend image.

---

### install_keys

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

List of signing keys for install_sources package sources, per charmhelpers standard format (a yaml list of strings encoded as a string). The keys should be the full ASCII armoured GPG public keys. While GPG key ids are also supported and looked up on a keyserver, operators should be aware that this mechanism is insecure. null can be used if a standard package signing key is used that will already be installed on the machine, and for PPA sources where the package signing key is securely retrieved from Launchpad.

---

### install_sources

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

List of extra apt sources, per charm-helpers standard format (a yaml list of strings encoded as a string). Each source may be either a line that can be added directly to sources.list(5), or in the form ppa:<user>/<ppa-name> for adding Personal Package Archives, or a distribution component to enable.

---

### keystone-policy

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-auth-policy
  namespace: kube-system
  labels:
    k8s-app: k8s-keystone-auth
data:
  policies: |
    [
      {
       "resource": {
          "verbs": ["get", "list", "watch"],
          "resources": ["*"],
          "version": "*",
          "namespace": "*"
        },
        "match": [
          {
            "type": "role",
            "values": ["k8s-viewers"]
          },
          {
            "type": "project",
            "values": ["k8s"]
          }
        ]
      },
      {
       "resource": {
          "verbs": ["*"],
          "resources": ["*"],
          "version": "*",
          "namespace": "default"
        },
        "match": [
          {
            "type": "role",
            "values": ["k8s-users"]
          },
          {
            "type": "project",
            "values": ["k8s"]
          }
        ]
      },
      {
       "resource": {
          "verbs": ["*"],
          "resources": ["*"],
          "version": "*",
          "namespace": "*"
        },
        "match": [
          {
            "type": "role",
            "values": ["k8s-admins"]
          },
          {
            "type": "project",
            "values": ["k8s"]
          }
        ]
      }
    ]


Policy for Keystone authentication. This is used when a Keystone charm is
related to kubernetes-master in order to provide authentication and authorization
for Keystone users on the Kubernetes cluster.

---

### keystone-ssl-ca

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Keystone certificate authority encoded in base64 for securing communications to Keystone.
For example: `juju config kubernetes-master keystone-ssl-ca=$(base64 /path/to/ca.crt)`

---

### loadbalancer-ips

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space separated list of IP addresses of loadbalancers in front of the control plane.
These can be either virtual IP addresses that have been floated in front of the control
plane or the IP of a loadbalancer appliance such as an F5. Workers will alternate IP
addresses from this list to distribute load - for example If you have 2 IPs and 4 workers,
each IP will be used by 2 workers. Note that this will only work if kubeapi-load-balancer
is not in use and there is a relation between kubernetes-master:kube-api-endpoint and
kubernetes-worker:kube-api-endpoint. If using the kubeapi-load-balancer, see the
loadbalancer-ips configuration variable on the kubeapi-load-balancer charm.

---

### monitoring-storage

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: influxdb:
  emptyDir: {}
grafana:
  emptyDir: {}


Configuration to set up volume for influxdb/grafana.
e.g
influxdb:
  hostPath:
    path: /influxdb
    type: Directory
grafana:
  hostPath:
    path: /grafana
    type: Directory

---

### nagios_context

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: juju

Used by the nrpe subordinate charms.
A string that will be prepended to instance name to set the host name
in nagios. So for instance the hostname would be something like:
    juju-myservice-0
If you're running multiple environments with the same services in them
this allows you to differentiate between them.

---

### nagios_servicegroups

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

A comma-separated list of nagios servicegroups.
If left empty, the nagios_context will be used as the servicegroup

---

### package_status

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: install

The status of service-affecting packages will be set to this value in the dpkg database. Valid values are "install" and "hold".

---

### proxy-extra-args

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space separated list of flags and key=value pairs that will be passed as arguments to
kube-proxy. For example a value like this:
  runtime-config=batch/v2alpha1=true profiling=true
will result in kube-apiserver being run with the following options:
  --runtime-config=batch/v2alpha1=true --profiling=true

---

### require-manual-upgrade

**Type**: boolean &nbsp;&nbsp;&nbsp; **Default**: True

When true, master nodes will not be upgraded until the user triggers
it manually by running the upgrade action.

---

### scheduler-extra-args

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

Space separated list of flags and key=value pairs that will be passed as arguments to
kube-scheduler. For example a value like this:
  runtime-config=batch/v2alpha1=true profiling=true
will result in kube-scheduler being run with the following options:
  --runtime-config=batch/v2alpha1=true --profiling=true

---

### service-cidr

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: 10.152.183.0/24

CIDR to user for Kubernetes services. Cannot be changed after deployment.
---

### snap_proxy

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

DEPRECATED. Use snap-http-proxy and snap-https-proxy model configuration settings. HTTP/HTTPS web proxy for Snappy to use when accessing the snap store.

---

### snap_proxy_url

**Type**: string &nbsp;&nbsp;&nbsp; **Default**:

DEPRECATED. Use snap-store-proxy model configuration setting. The address of a Snap Store Proxy to use for snaps e.g. http://snap-proxy.example.com

---

### snapd_refresh

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: max

How often snapd handles updates for installed snaps. Setting an empty
string will check 4x per day. Set to "max" to delay the refresh as long
as possible. You may also set a custom string as described in the
'refresh.timer' section here:
  https://forum.snapcraft.io/t/system-options/87

---

### storage-backend

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: auto

The storage backend for kube-apiserver persistence. Can be "etcd2", "etcd3", or
"auto". Auto mode will select etcd3 on new installations, or etcd2 on upgrades.

---

### sysctl

**Type**: string &nbsp;&nbsp;&nbsp; **Default**: { net.ipv4.conf.all.forwarding : 1, net.ipv4.neigh.default.gc_thresh1 : 128, net.ipv4.neigh.default.gc_thresh2 : 28672, net.ipv4.neigh.default.gc_thresh3 : 32768, net.ipv6.neigh.default.gc_thresh1 : 128, net.ipv6.neigh.default.gc_thresh2 : 28672, net.ipv6.neigh.default.gc_thresh3 : 32768, fs.inotify.max_user_instances : 8192, fs.inotify.max_user_watches: 1048576 }

YAML formatted associative array of sysctl values, e.g.:
'{kernel.pid_max : 4194303 }'. Note that kube-proxy handles
the conntrack settings. The proper way to alter them is to
use the proxy-extra-args config to set them, e.g.:
  juju config kubernetes-master proxy-extra-args="conntrack-min=1000000 conntrack-max-per-core=250000"
  juju config kubernetes-worker proxy-extra-args="conntrack-min=1000000 conntrack-max-per-core=250000"
The proxy-extra-args conntrack-min and conntrack-max-per-core can be set to 0 to ignore
kube-proxy's settings and use the sysctl settings instead. Note the fundamental difference between
the setting of conntrack-max-per-core vs nf_conntrack_max.

---


<!-- CONFIG ENDS -->


## kubernetes-master charm actions
