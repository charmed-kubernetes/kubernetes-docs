<p>On-disk files in a container are ephemeral and can’t be shared with other members of a pod. For some applications, this is not an issue, but for many persistent storage is required.</p>

<p><strong>Charmed Kubernetes</strong> makes it easy to add and configure different types of persistent storage for your <strong>Kubernetes</strong> cluster, as outlined below. For more detail on the concept of storage volumes in <strong>Kubernetes</strong>, please see the <a href="https://kubernetes.io/docs/concepts/storage/">Kubernetes documentation</a>.</p>

<h2 id="ceph-storage">Ceph storage</h2>

<p><strong>Charmed Kubernetes</strong> can make use of <a href="https://ceph.com/">Ceph</a> to provide persistent storage
volumes. The following sections assume you have already deployed a
<strong>Charmed Kubernetes</strong> cluster and you have internet access to the
<strong>Juju</strong> Charm Store.</p>

<h3 id="deploy-ceph">Deploy Ceph</h3>

<p>Check that the current <strong>Juju</strong> model is the one where you wish to deploy <strong>Ceph</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju switch
</code></pre></div></div>

<p>Begin by adding a minimum number of <strong>Ceph</strong> monitor nodes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> juju deploy <span class="nt">-n</span> 3 ceph-mon
</code></pre></div></div>

<p>For the storage nodes we will also need to specify storage volumes for the backing cloud to add. This is done by using the <code class="highlighter-rouge">--storage</code> option. The <a href="https://jujucharms.com/ceph-osd/"><code class="highlighter-rouge">ceph-osd</code> charm</a> defines two useful types of storage, <code class="highlighter-rouge">osd-devices</code> for the volumes which will be formatted and used to provide storage, and <code class="highlighter-rouge">osd-journals</code> for storage used for journalling.</p>

<p>The format for the <code class="highlighter-rouge">--storage</code> option is <code class="highlighter-rouge">&lt;storage pool&gt;,&lt;size&gt;,&lt;number&gt;</code>. The storage pools available are dependent on and defined by the backing cloud. However, by omitting the storage type, the default pool for that cloud will be chosen (E.g. for AWS, the default pool is EBS storage).</p>

<p>So, for example, to deploy three <code class="highlighter-rouge">ceph-osd</code> storage nodes, using the default storage pool, with 2x 32G volumes of storage per node, and one 8G journal, we would use the command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> juju deploy <span class="nt">-n</span> 3 ceph-osd <span class="nt">--storage</span> osd-devices<span class="o">=</span>32G,2 <span class="nt">--storage</span> osd-journals<span class="o">=</span>8G,1
</code></pre></div></div>

<div class="p-notification--positive"><p class="p-notification__response">
<span class="p-notification__status">Note:</span>
For a more detailed explanation of Juju’s storage pools and options, please see the relevant <a href="https://docs.jujucharms.com/stable/en/charms-storage">Juju documentation</a>.
</p></div>

<p>Note that actually deploying these charms with storage may take some time, but you can continue to run other Juju commands in the meantime.</p>

<p>The <code class="highlighter-rouge">ceph-osd</code> and <code class="highlighter-rouge">ceph-mon</code> deployments should then be connected:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju add-relation ceph-osd ceph-mon
</code></pre></div></div>

<h3 id="relate-to-charmed-kubernetes">Relate to Charmed Kubernetes</h3>

<p>Making <strong>Charmed Kubernetes</strong> aware of your <strong>Ceph</strong> cluster requires 2 <strong>Juju</strong> relations.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju add-relation ceph-mon:admin kubernetes-master
juju add-relation ceph-mon:client kubernetes-master
</code></pre></div></div>

<h3 id="create-storage-pools">Create storage pools</h3>

<p>By default, the <code class="highlighter-rouge">kubernetes-master</code> charm will create the required pools defined
in the storage class.  To view the default options, run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju list-actions ceph-mon <span class="nt">--schema</span> <span class="nt">--format</span> json | jq <span class="s1">'.["create-pool"]'</span>
</code></pre></div></div>

<p>If you’re happy with this, you can skip the section.  Otherwise, if you want to
change these, you can delete the pools:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju run <span class="nt">--unit</span> ceph-mon/0 <span class="s2">"ceph tell mon.</span><span class="se">\*</span><span class="s2"> injectargs '--mon-allow-pool-delete=true'"</span>

juju run-action ceph-mon/0 delete-pool pool-name<span class="o">=</span>xfs-pool <span class="nt">--wait</span>
juju run-action ceph-mon/0 delete-pool pool-name<span class="o">=</span>ext4-pool <span class="nt">--wait</span>
</code></pre></div></div>

<p>Then recreate them, using the options listed from the <code class="highlighter-rouge">list-actions</code> command ran
earlier.  For example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju run-action ceph-mon/0 create-pool <span class="nv">name</span><span class="o">=</span>xfs-pool <span class="nv">replicas</span><span class="o">=</span>6 <span class="nt">--wait</span>
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">unit-ceph-mon-0</span><span class="pi">:</span>
  <span class="na">id</span><span class="pi">:</span> <span class="s">c12f0688-f31b-4956-8314-abacd2d6516f</span>
  <span class="na">status</span><span class="pi">:</span> <span class="s">completed</span>
  <span class="na">timing</span><span class="pi">:</span>
    <span class="na">completed</span><span class="pi">:</span> <span class="s">2018-08-20 20:49:34 +0000 UTC</span>
    <span class="na">enqueued</span><span class="pi">:</span> <span class="s">2018-08-20 20:49:31 +0000 UTC</span>
    <span class="na">started</span><span class="pi">:</span> <span class="s">2018-08-20 20:49:31 +0000 UTC</span>
  <span class="na">unit</span><span class="pi">:</span> <span class="s">ceph-mon/0</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju run-action ceph-mon/0 create-pool <span class="nv">name</span><span class="o">=</span>ext4-pool <span class="nv">replicas</span><span class="o">=</span>6 <span class="nt">--wait</span>
</code></pre></div></div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">unit-ceph-mon-0</span><span class="pi">:</span>
  <span class="na">id</span><span class="pi">:</span> <span class="s">4e82d93d-546f-441c-89e1-d36152c082f2</span>
  <span class="na">status</span><span class="pi">:</span> <span class="s">completed</span>
  <span class="na">timing</span><span class="pi">:</span>
    <span class="na">completed</span><span class="pi">:</span> <span class="s">2018-08-20 20:49:45 +0000 UTC</span>
    <span class="na">enqueued</span><span class="pi">:</span> <span class="s">2018-08-20 20:49:41 +0000 UTC</span>
    <span class="na">started</span><span class="pi">:</span> <span class="s">2018-08-20 20:49:43 +0000 UTC</span>
  <span class="na">unit</span><span class="pi">:</span> <span class="s">ceph-mon/0</span>
</code></pre></div></div>

<h3 id="verification">Verification</h3>

<p>Now you can look at your <strong>Charmed Kubernetes</strong> cluster to verify things are working. Running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get sc,po
</code></pre></div></div>

<p>… should return output similar to:</p>

<pre><code class="language-no-highlight">NAME                                             PROVISIONER     AGE
storageclass.storage.k8s.io/ceph-ext4            csi-rbdplugin    7m
storageclass.storage.k8s.io/ceph-xfs (default)   csi-rbdplugin    7m

NAME                                                   READY     STATUS    RESTARTS   AGE
pod/csi-rbdplugin-attacher-0                           1/1       Running   0          7m
pod/csi-rbdplugin-cnh9k                                2/2       Running   0          7m
pod/csi-rbdplugin-lr66m                                2/2       Running   0          7m
pod/csi-rbdplugin-mnn94                                2/2       Running   0          7m
pod/csi-rbdplugin-provisioner-0                        1/1       Running   0          7m
</code></pre>

<h3 id="scaling-out">Scaling out</h3>

<p>To check existing storage allocation, use the command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju storage
</code></pre></div></div>

<p>If extra storage is required, it is possible to add extra <code class="highlighter-rouge">ceph-osd</code> units as
desired:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju add-unit ceph-osd <span class="nt">-n</span> 2
</code></pre></div></div>

<p>Once again, it is necessary to attach appropriate storage volumes as before. In this case though, the storage needs to be added on a per-unit basis.</p>

<p>Confirm the running units of <code class="highlighter-rouge">ceph-osd</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju status ceph-osd
</code></pre></div></div>

<p>Add additional storage to existing or new units with the <code class="highlighter-rouge">add-storage</code> command. For example, to add two volumes of 32G to the unit <code class="highlighter-rouge">ceph-osd/2</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju add-storage ceph-osd/2 <span class="nt">--storage</span> osd-devices<span class="o">=</span>32G,2
</code></pre></div></div>

<h3 id="using-a-separate-juju-model">Using a separate Juju model</h3>

<p>In some circumstances it can be useful to locate the persistent storage in a different <strong>Juju</strong> model, for example to have one set of storage used by different clusters. The only change required is in adding relations between <strong>Ceph</strong> and <strong>Charmed Kubernetes</strong>.</p>

<p>For more information on how to achieve this, please see the <a href="https://docs.jujucharms.com/stable/en/models-cmr">Juju documentation</a> on cross-model relations.</p>

<h2 id="nfs">NFS</h2>

<p>It is possible to add simple storage for <strong>Kubernetes</strong> using NFS. In this case, the storage is implemented on the root disk of units running the <code class="highlighter-rouge">nfs</code> charm.</p>

<h3 id="deploy-nfs">Deploy NFS</h3>

<p>Make use of <strong>Juju</strong> constraints to allocate an instance with the required amount of storage. For example, for 200G of storage:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju deploy nfs <span class="nt">--constraints</span> root-disk<span class="o">=</span>200G
</code></pre></div></div>

<h3 id="relate-to-charmed-kubernetes-1">Relate to Charmed Kubernetes</h3>

<p>The NFS units can be related directly to the <strong>Kubernetes</strong> workers:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> juju add-relation nfs kubernetes-worker
</code></pre></div></div>

<h3 id="verification-1">Verification</h3>

<p>Now you can look at your <strong>Charmed Kubernetes</strong> cluster to verify things
are working. Running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get sc,po
</code></pre></div></div>

<p>… should return output similar to:</p>

<pre><code class="language-no-highlight">NAME                                            PROVISIONER      AGE
storageclass.storage.k8s.io/default (default)   fuseim.pri/ifs   3m

NAME                                                   READY     STATUS    RESTARTS   AGE
pod/nfs-client-provisioner-778dcffbc8-2725b            1/1       Running   0          3m
</code></pre>

<h3 id="scaling-out-1">Scaling out</h3>

<p>If extra storage is required, it is possible to add extra <code class="highlighter-rouge">nfs</code> units as desired. For example, to add three new units, each with 100G of storage:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju add-unit nfs  <span class="nt">-n</span> 3 <span class="nt">--constraints</span> root-disk<span class="o">=</span>100G
</code></pre></div></div>

<p>There is no requirement that these additional units should have the same amount of storage space as previously.</p>

<!-- LINKS -->

<!-- FEEDBACK -->
<div class="p-notification--information">
  <p class="p-notification__response">
    We appreciate your feedback on the documentation. You can 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/edit/master/pages/k8s/storage.md" class="p-notification__action">edit this page</a> 
    or 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/issues/new" class="p-notification__action">file a bug here</a>.
  </p>
</div>
