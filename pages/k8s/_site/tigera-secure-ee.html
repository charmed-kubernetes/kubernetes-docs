<p>Tigera Secure EE is a software-defined network solution that can be used with
Kubernetes. For those familiar with Calico, Tigera Secure EE is essentially
Calico with enterprise features on top.</p>

<p>Support for Tigera Secure EE in <strong>Charmed Kubernetes</strong> is provided in the form of a
<code class="highlighter-rouge">tigera-secure-ee</code> subordinate charm, which can be used instead of <code class="highlighter-rouge">flannel</code> or
<code class="highlighter-rouge">calico</code>.</p>

<h2 id="deploying-charmed-kubernetes-with-tigera-secure-ee">Deploying Charmed Kubernetes with Tigera Secure EE</h2>

<p>Before you start, you will need:</p>

<ul>
  <li>Tigera Secure EE licence key</li>
  <li>Tigera private Docker registry credentials (provided as a Docker config.json)</li>
</ul>

<div class="p-notification--information">
  <p class="p-notification__response">
    <span class="p-notification__status">Note:</span>
    Tigera Secure EE's network traffic, much like Calico's, is filtered on
    many clouds. It will work on MAAS, and can work on AWS if you manually
    configure instances to disable source/destination checking.
  </p>
</div>

<p>To start, deploy <strong>Charmed Kubernetes</strong> with Tigera Secure EE:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju deploy cs:~containers/kubernetes-tigera-secure-ee
</code></pre></div></div>

<p>Configure the <code class="highlighter-rouge">tigera-secure-ee</code> charm with your licence key and registry
credentials:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju config tigera-secure-ee <span class="se">\</span>
  license-key<span class="o">=</span><span class="k">$(</span><span class="nb">base64</span> <span class="nt">-w0</span> license.yaml<span class="k">)</span> <span class="se">\</span>
  registry-credentials<span class="o">=</span><span class="k">$(</span><span class="nb">base64</span> <span class="nt">-w0</span> config.json<span class="k">)</span>
</code></pre></div></div>

<p>Wait for the deployment to settle before continuing on.</p>

<h2 id="using-the-built-in-elasticsearch-operator">Using the built-in elasticsearch-operator</h2>

<div class="p-notification--caution">
  <p class="p-notification__response">
    <span class="p-notification__status">Caution:</span>
     The built-in elasticsearch-operator is only recommended for testing or demonstrative
     purposes. For production deployments, please skip down to the next section.  </p>
</div>

<p>For testing and quick start purposes, the <code class="highlighter-rouge">tigera-secure-ee</code> charm deploys
<a href="https://github.com/upmc-enterprises/elasticsearch-operator">elasticsearch-operator</a> into your Kubernetes cluster by default. For it to
properly work, you will need to create a StorageClass.</p>

<p>The easiest way to do this is with the hostpath provisioner. Create a file named
<code class="highlighter-rouge">elasticsearch-storage.yaml</code> containing the following:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This manifest implements elasticsearch-storage using local host-path volumes.</span>
<span class="c1"># It is not suitable for production use; and only works on single node clusters.</span>

<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">elasticsearch-storage</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">storageclass.kubernetes.io/is-default-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">kubernetes.io/host-path</span>

<span class="nn">---</span>

<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tigera-elasticsearch-1</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">5Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/var/tigera/elastic-data/1</span>
  <span class="na">persistentVolumeReclaimPolicy</span><span class="pi">:</span> <span class="s">Recycle</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">elasticsearch-storage</span>

<span class="nn">---</span>

<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">tigera-elasticsearch-2</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">5Gi</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">hostPath</span><span class="pi">:</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s">/var/tigera/elastic-data/2</span>
  <span class="na">persistentVolumeReclaimPolicy</span><span class="pi">:</span> <span class="s">Recycle</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">elasticsearch-storage</span>
</code></pre></div></div>

<p>Apply elasticsearch-storage.yaml:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> elasticsearch-storage.yaml
</code></pre></div></div>

<p>Once you have a StorageClass available, delete the existing PVC and pods so
Kubernetes will recreate them using the new StorageClass:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl delete pvc <span class="nt">-n</span> calico-monitoring es-data-es-data-tigera-elasticsearch-default-0
kubectl delete pvc <span class="nt">-n</span> calico-monitoring es-data-es-master-tigera-elasticsearch-default-0
kubectl delete po <span class="nt">-n</span> calico-monitoring es-data-tigera-elasticsearch-default-0
kubectl delete po <span class="nt">-n</span> calico-monitoring es-master-tigera-elasticsearch-default-0
</code></pre></div></div>

<p>For a more robust storage solution, consider deploying Ceph with <strong>Charmed Kubernetes</strong>, as
documented in the <a href="/kubernetes/docs/storage">Storage</a> section. This will create a default StorageClass
that elasticsearch-operator will use automatically.</p>

<h2 id="using-your-own-elasticsearch">Using your own ElasticSearch</h2>

<p>Disable the built-in elasticsearch operator:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju config tigera-secure-ee enable-elasticsearch-operator<span class="o">=</span><span class="nb">false</span>
</code></pre></div></div>

<p>Then follow this guide from Tigera:
<a href="https://docs.tigera.io/v2.2/getting-started/kubernetes/installation/byo-elasticsearch">Using your own ElasticSearch for logs</a></p>

<h2 id="accessing-cnx-manager">Accessing cnx-manager</h2>

<p>The cnx-manager service is exposed as a NodePort on port 30003. Run the
following command to open port 30003 on the workers:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju run <span class="nt">--application</span> kubernetes-worker open-port 30003
</code></pre></div></div>

<p>Then connect to <code class="highlighter-rouge">https://&lt;kubernetes-worker-ip&gt;:30003</code> in your web browser. Use
the Kubernetes admin credentials to log in (you can find these in the kubeconfig
file created on kubernetes-master units at <code class="highlighter-rouge">/home/ubuntu/config</code>).</p>

<h2 id="accessing-kibana">Accessing kibana</h2>

<p>The kibana service is exposed as a NodePort on port 30601. Run the following
command to open port 30601 on the workers:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju run <span class="nt">--application</span> kubernetes-worker open-port 30601
</code></pre></div></div>

<div class="p-notification--caution">
  <p class="p-notification__response">
    <span class="p-notification__status">Caution:</span>
    Do not open this port if your kubernetes-worker units are exposed on a
    network you do not trust. Kibana does not require credentials to use</p>
</div>

<p>Then connect to <code class="highlighter-rouge">http://&lt;kubernetes-worker-ip&gt;:30601</code> in your web browser.</p>

<h2 id="using-a-private-docker-registry">Using a private Docker registry</h2>

<p>For a general introduction to using a private Docker registry with
<strong>Charmed Kubernetes</strong>, please refer to the <a href="/kubernetes/docs/docker-registry">Private Docker Registry</a> page.</p>

<p>In addition to the steps documented there, you will need to upload the
following images to the registry:</p>

<pre><code class="language-no-highlight">docker.elastic.co/elasticsearch/elasticsearch-oss:6.4.3
docker.elastic.co/kibana/kibana-oss:6.4.3
quay.io/tigera/calicoctl:v2.3.0
quay.io/tigera/calicoq:v2.3.0
quay.io/tigera/cnx-apiserver:v2.3.0
quay.io/tigera/cnx-manager:v2.3.0
quay.io/tigera/cnx-manager-proxy:v2.3.0
quay.io/tigera/cnx-node:v2.3.0
quay.io/tigera/cnx-queryserver:v2.3.0
quay.io/tigera/es-proxy:v2.3.0
quay.io/tigera/fluentd:v2.3.0
quay.io/tigera/kube-controllers:v2.3.0
quay.io/tigera/cloud-controllers:v2.3.0
quay.io/tigera/typha:v2.3.0
quay.io/tigera/intrusion-detection-job-installer:v2.3.0
quay.io/tigera/es-curator:v2.3.0
quay.io/coreos/configmap-reload:v0.0.1
quay.io/coreos/prometheus-config-reloader:v0.0.3
quay.io/coreos/prometheus-operator:v0.18.1
quay.io/prometheus/alertmanager:v0.14.0
quay.io/prometheus/prometheus:v2.2.1
docker.io/upmcenterprises/elasticsearch-operator:0.2.0
docker.io/busybox:latest
docker.io/alpine:3.7
</code></pre>

<p>And configure Tigera Secure EE to use the registry with this shell script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">IP</span><span class="o">=</span><span class="sb">`</span>juju run <span class="nt">--unit</span> docker-registry/0 <span class="s1">'network-get website --ingress-address'</span><span class="sb">`</span>
<span class="nb">export </span><span class="nv">PORT</span><span class="o">=</span><span class="sb">`</span>juju config docker-registry registry-port<span class="sb">`</span>
<span class="nb">export </span><span class="nv">REGISTRY</span><span class="o">=</span><span class="nv">$IP</span>:<span class="nv">$PORT</span>
juju config tigera-secure-ee <span class="nv">registry</span><span class="o">=</span><span class="nv">$REGISTRY</span>
</code></pre></div></div>

<!-- LINKS -->

<!-- FEEDBACK -->
<div class="p-notification--information">
  <p class="p-notification__response">
    We appreciate your feedback on the documentation. You can 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/edit/master/pages/k8s/tigera-secure-ee.md" class="p-notification__action">edit this page</a> 
    or 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/issues/new" class="p-notification__action">file a bug here</a>.
  </p>
</div>
