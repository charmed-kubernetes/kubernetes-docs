<p><strong>Charmed Kubernetes</strong> will run seamlessly on
<strong>Google Cloud Platform</strong>(GCP).  With the addition of the <code class="highlighter-rouge">gcp-integrator</code>,
your cluster will also be able to use GCP native features directly.</p>

<h2 id="gcp-credentials">GCP Credentials</h2>

<p>If you have set up a service account with IAM roles as your credential for
Juju, there may be some additional authorisations you will need to make to
access all features of GCP with <strong>Charmed Kubernetes</strong>.</p>

<p>If you have a GCP project set up specifically for <strong>Charmed Kubernetes</strong>, the
quickest route is to simply add the service account as an <code class="highlighter-rouge">Owner</code> of that
project in the <a href="https://console.cloud.google.com/iam-admin/iam">GCP console</a>.</p>

<p>If you chose a more fine-grained approach to role administration, the service account
should have at least:</p>

<ul>
  <li>roles/compute.loadBalancerAdmin</li>
  <li>roles/compute.instanceAdmin.v1</li>
  <li>roles/compute.securityAdmin</li>
  <li>roles/iam.serviceAccountUser</li>
</ul>

<p>A full description of the various pre-defined roles is available in the
<a href="https://cloud.google.com/compute/docs/access/iam">GCP Documentation</a>.</p>

<h2 id="gcp-integrator">GCP integrator</h2>

<p>The <code class="highlighter-rouge">gcp-integrator</code> charm simplifies working with <strong>Charmed Kubernetes</strong> on
GCP. Using the credentials provided to Juju, it acts as a proxy between
<strong>Charmed Kubernetes</strong> and the underlying cloud, granting permissions to
dynamically create, for example, storage volumes.</p>

<h3 id="installing">Installing</h3>

<p>If you install <strong>Charmed Kubernetes</strong> <a href="/kubernetes/docs/install-manual">using the Juju bundle</a>, you can add the
gcp-integrator at the same time by using the following overlay file (<a href="https://raw.githubusercontent.com/charmed-kubernetes/bundle/master/overlays/gcp-overlay.yaml">download
it here</a>):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">description</span><span class="pi">:</span> <span class="s">Charmed Kubernetes overlay to add native GCP support.</span>
<span class="na">applications</span><span class="pi">:</span>
  <span class="na">gcp-integrator</span><span class="pi">:</span>
    <span class="na">annotations</span><span class="pi">:</span>
      <span class="na">gui-x</span><span class="pi">:</span> <span class="s2">"</span><span class="s">600"</span>
      <span class="na">gui-y</span><span class="pi">:</span> <span class="s2">"</span><span class="s">300"</span>
    <span class="na">charm</span><span class="pi">:</span> <span class="s">cs:~containers/gcp-integrator</span>
    <span class="na">num_units</span><span class="pi">:</span> <span class="s">1</span>
    <span class="na">trust</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">relations</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="pi">[</span><span class="s1">'</span><span class="s">gcp-integrator'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">kubernetes-master'</span><span class="pi">]</span>
  <span class="pi">-</span> <span class="pi">[</span><span class="s1">'</span><span class="s">gcp-integrator'</span><span class="pi">,</span> <span class="s1">'</span><span class="s">kubernetes-worker'</span><span class="pi">]</span>
</code></pre></div></div>

<p>To use this overlay with the <strong>Charmed Kubernetes</strong> bundle, it is specified
during deploy like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju deploy charmed-kubernetes <span class="nt">--overlay</span> ~/path/gcp-overlay.yaml
</code></pre></div></div>

<p>Then run the command to share credentials with this charm:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju trust gcp-integrator
</code></pre></div></div>

<p>… and remember to fetch the configuration file!</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju scp kubernetes-master/0:config ~/.kube/config
</code></pre></div></div>

<p>For more configuration options and details of the permissions which the
integrator uses, please see the <a href="https://jujucharms.com/u/containers/gcp-integrator/">charm readme</a>.</p>

<h3 id="using-persistent-storage">Using persistent storage</h3>

<p>Many pods you may wish to deploy will require storage. Although you can use any
type of storage supported by Kubernetes (see the
<a href="/kubernetes/docs/storage">storage documentation</a>), you also have the option to use the native
GCP storage types.</p>

<p>GCP storage currently comes in two types - SSD (pd-ssd) or
‘standard’(pd-standard). To use these, we need to create a storage classes in
Kubernetes.</p>

<p>For the standard disks:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOY</span><span class="sh">
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gcp-standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
</span><span class="no">EOY
</span></code></pre></div></div>

<p>Or for SSD:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOY</span><span class="sh">
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gcp-ssd
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
</span><span class="no">EOY
</span></code></pre></div></div>

<p>You can confirm this has been added by running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get sc
</code></pre></div></div>

<p>which should return:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME           PROVISIONER            AGE
gcp-ssd        kubernetes.io/gce-pd   9s
gcp-standard   kubernetes.io/gce-pd   45s
</code></pre></div></div>

<p>To actually create storage using this new class, you can make a Persistent Volume Claim:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOY</span><span class="sh">
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: testclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
  storageClassName: gcp-standard
</span><span class="no">EOY
</span></code></pre></div></div>

<p>This should finish with a confirmation. You can check the current PVCs with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pvc
</code></pre></div></div>

<p>…which should return something similar to:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
testclaim   Bound    pvc-e1d42bae-44e6-11e9-8dff-42010a840007   1Gi        RWO            gcp-standard   15s
</code></pre></div></div>

<p>This PVC can then be used by pods operating in the cluster. As an example, the following
deploys a <code class="highlighter-rouge">busybox</code> pod:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl create <span class="nt">-f</span> - <span class="o">&lt;&lt;</span><span class="no">EOY</span><span class="sh">
apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
    - image: busybox
      command:
        - sleep
        - "3600"
      imagePullPolicy: IfNotPresent
      name: busybox
      volumeMounts:
        - mountPath: "/pv"
          name: testvolume
  restartPolicy: Always
  volumes:
    - name: testvolume
      persistentVolumeClaim:
        claimName: testclaim
</span><span class="no">EOY
</span></code></pre></div></div>

<p>To set this type of storage as the default, you can use the command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl patch storageclass gcp-standard <span class="nt">-p</span> <span class="s1">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span>
</code></pre></div></div>

<div class="p-notification--caution">
  <p class="p-notification__response">
    <span class="p-notification__status">Note:</span>
If you create persistent disks and subsequently tear down the cluster, check
with the GCP console to make sure all the associated resources have also been released.
  </p>
</div>

<h3 id="using-gcp-loadbalancers">Using GCP Loadbalancers</h3>

<p>With the gcp-integrator charm in place, actions which invoke a loadbalancer in
Kubernetes  will automatically generate a GCP <a href="https://cloud.google.com/load-balancing/docs/target-pools">Target Pool</a> and the
relevant forwarding rules.  This can be demonstrated with a simple application. Here we
will create an application running in five pods:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl run hello-world <span class="nt">--replicas</span><span class="o">=</span>5 <span class="nt">--labels</span><span class="o">=</span><span class="s2">"run=load-balancer-example"</span> <span class="nt">--image</span><span class="o">=</span>gcr.io/google-samples/node-hello:1.0  <span class="nt">--port</span><span class="o">=</span>8080
</code></pre></div></div>

<p>You can verify that the application and replicas have been created with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl get deployments hello-world
</code></pre></div></div>

<p>Which should return output similar to:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> NAME              READY   UP-TO-DATE   AVAILABLE   AGE
 hello-world      5/5               5                            5             2m38s
</code></pre></div></div>

<p>To create a target pool load balancer, the application should now be exposed as
a service:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> kubectl expose deployment hello-world <span class="nt">--type</span><span class="o">=</span>LoadBalancer <span class="nt">--name</span><span class="o">=</span>hello
</code></pre></div></div>

<p>To check that the service is running correctly:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl describe service hello
</code></pre></div></div>

<p>…which should return output similar to:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Name</span><span class="pi">:</span>                     <span class="s">hello</span>
<span class="na">Namespace</span><span class="pi">:</span>                <span class="s">default</span>
<span class="na">Labels</span><span class="pi">:</span>                   <span class="s">run=load-balancer-example</span>
<span class="na">Annotations</span><span class="pi">:</span>              <span class="s">&lt;none&gt;</span>
<span class="na">Selector</span><span class="pi">:</span>                 <span class="s">run=load-balancer-example</span>
<span class="na">Type</span><span class="pi">:</span>                     <span class="s">LoadBalancer</span>
<span class="na">IP</span><span class="pi">:</span>                       <span class="s">10.152.183.63</span>
<span class="na">LoadBalancer Ingress</span><span class="pi">:</span>     <span class="s">34.76.144.215</span>
<span class="na">Port</span><span class="pi">:</span>                     <span class="s">&lt;unset&gt;  8080/TCP</span>
<span class="na">TargetPort</span><span class="pi">:</span>               <span class="s">8080/TCP</span>
<span class="na">NodePort</span><span class="pi">:</span>                 <span class="s">&lt;unset&gt;  31864/TCP</span>
<span class="na">Endpoints</span><span class="pi">:</span>                <span class="s">10.1.54.11:8080,10.1.54.12:8080,10.1.54.13:8080 + 2 more...</span>
<span class="na">Session Affinity</span><span class="pi">:</span>         <span class="s">None</span>
<span class="na">External Traffic Policy</span><span class="pi">:</span>  <span class="s">Cluster</span>
<span class="na">Events</span><span class="pi">:</span>
  <span class="s">Type    Reason                Age    From                Message</span>
  <span class="s">----    ------                ----   ----                -------</span>
  <span class="s">Normal  EnsuringLoadBalancer  9m21s  service-controller  Ensuring load balancer</span>
  <span class="s">Normal  EnsuredLoadBalancer   7m37s  service-controller  Ensured load balancer</span>

</code></pre></div></div>

<p>You can see that the <code class="highlighter-rouge">LoadBalancer Ingress</code> is now associated with a new
ingress address in front of the five endpoints of the  example deployment. You
can test this address:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl 34.76.144.215:8080
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hello Kubernetes!
</code></pre></div></div>

<h3 id="upgrading-the-integrator-charm">Upgrading the integrator-charm</h3>

<p>The gcp-integrator is not specifically tied to the version of <strong>Charmed Kubernetes</strong> installed and may
generally be upgraded at any time with the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju upgrade-charm gcp-integrator
</code></pre></div></div>

<h3 id="troubleshooting">Troubleshooting</h3>

<p>If you have any specific problems with the gcp-integrator, you can report bugs on
<a href="https://bugs.launchpad.net/charmed-kubernetes">Launchpad</a>.</p>

<p>Any activity in GCP can be monitored from the <a href="https://console.cloud.google.com/compute/operations">Operations</a> console.
If you are using a service account with IAM roles, it is relatively easy to see
the actions that particular account is responsible for.</p>

<p>For logs of what the charm itself believes the world to look like, you can use
Juju to replay the log history for that specific unit:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju debug-log <span class="nt">--replay</span> <span class="nt">--include</span> gcp-integrator/0
</code></pre></div></div>

<!-- LINKS -->

<!-- FEEDBACK -->
<div class="p-notification--information">
  <p class="p-notification__response">
    We appreciate your feedback on the documentation. You can 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/edit/master/pages/k8s/gcp-integration.md" class="p-notification__action">edit this page</a> 
    or 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/issues/new" class="p-notification__action">file a bug here</a>.
  </p>
</div>
