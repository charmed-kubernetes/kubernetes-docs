<p>This document covers how to troubleshoot the deployment of a Kubernetes cluster, it will not cover debugging of workloads inside Kubernetes.</p>

<h2 id="understanding-cluster-status">Understanding Cluster Status</h2>

<p>Using <code class="highlighter-rouge">juju status</code> can give you some insight as to what’s happening in a cluster:</p>

<pre><code class="language-no-highlight">Model                         Controller          Cloud/Region   Version  SLA          Timestamp
conjure-canonical-kubern-ade  conjure-up-aws-91c  aws/eu-west-1  2.4.5    unsupported  08:38:09+01:00

App                    Version  Status  Scale  Charm                  Store       Rev  OS      Notes
aws-integrator         1.15.71  active      1  aws-integrator         jujucharms    5  ubuntu  
easyrsa                3.0.1    active      1  easyrsa                jujucharms  117  ubuntu  
etcd                   3.2.10   active      3  etcd                   jujucharms  209  ubuntu  
flannel                0.10.0   active      5  flannel                jujucharms  146  ubuntu  
kubeapi-load-balancer  1.14.0   active      1  kubeapi-load-balancer  jujucharms  162  ubuntu  exposed
kubernetes-master      1.12.1   active      2  kubernetes-master      jujucharms  219  ubuntu  
kubernetes-worker      1.12.1   active      3  kubernetes-worker      jujucharms  239  ubuntu  exposed

Unit                      Workload  Agent  Machine  Public address  Ports           Message
aws-integrator/0*         active    idle   0        54.171.121.229                  ready
easyrsa/0*                active    idle   1        34.251.192.5                    Certificate Authority connected.
etcd/0*                   active    idle   2        52.18.186.65    2379/tcp        Healthy with 3 known peers
etcd/1                    active    idle   3        54.194.35.197   2379/tcp        Healthy with 3 known peers
etcd/2                    active    idle   4        34.240.14.183   2379/tcp        Healthy with 3 known peers
kubeapi-load-balancer/0*  active    idle   5        34.244.110.15   443/tcp         Loadbalancer ready.
kubernetes-master/0*      active    idle   6        34.254.175.71   6443/tcp        Kubernetes master running.
  flannel/0*              active    idle            34.254.175.71                   Flannel subnet 10.1.16.1/24
kubernetes-master/1       active    idle   7        52.210.61.51    6443/tcp        Kubernetes master running.
  flannel/3               active    idle            52.210.61.51                    Flannel subnet 10.1.38.1/24
kubernetes-worker/0*      active    idle   8        34.246.168.241  80/tcp,443/tcp  Kubernetes worker running.
  flannel/1               active    idle            34.246.168.241                  Flannel subnet 10.1.79.1/24
kubernetes-worker/1       active    idle   9        54.229.236.169  80/tcp,443/tcp  Kubernetes worker running.
  flannel/4               active    idle            54.229.236.169                  Flannel subnet 10.1.10.1/24
kubernetes-worker/2       active    idle   10       34.253.203.147  80/tcp,443/tcp  Kubernetes worker running.
  flannel/2               active    idle            34.253.203.147                  Flannel subnet 10.1.95.1/24

Entity  Meter status  Message
model   amber         user verification pending  

Machine  State    DNS             Inst id              Series  AZ          Message
0        started  54.171.121.229  i-0f47fcfb452fa8fab  bionic  eu-west-1a  running
1        started  34.251.192.5    i-011007983db6d2736  bionic  eu-west-1b  running
2        started  52.18.186.65    i-0b411be2a3909ae32  bionic  eu-west-1a  running
3        started  54.194.35.197   i-0fccba854c6d59ffe  bionic  eu-west-1b  running
4        started  34.240.14.183   i-02148162336e08864  bionic  eu-west-1c  running
5        started  34.244.110.15   i-08833b743ebcd0d9c  bionic  eu-west-1c  running
6        started  34.254.175.71   i-0f18d3f7377ba406f  bionic  eu-west-1a  running
7        started  52.210.61.51    i-08ec1daf25fb18fa3  bionic  eu-west-1b  running
8        started  34.246.168.241  i-0934f74bfdfba2a3f  bionic  eu-west-1b  running
9        started  54.229.236.169  i-0a4129c834c713a5e  bionic  eu-west-1a  running
10       started  34.253.203.147  i-053492139b1080ce0  bionic  eu-west-1c  running
</code></pre>

<p>In this example we can glean some information. The <code class="highlighter-rouge">Workload</code> column will show the status of a given service. The <code class="highlighter-rouge">Message</code> section will show you the health of a given service in the cluster. During deployment and maintenance these workload statuses will update to reflect what a given node is doing. For example the workload may say <code class="highlighter-rouge">maintenance</code> while message will describe this maintenance as <code class="highlighter-rouge">Installing docker</code>.</p>

<p>During normal operation the Workload should read <code class="highlighter-rouge">active</code>, the Agent column (which reflects what the Juju agent is doing) should read <code class="highlighter-rouge">idle</code>, and the messages will either say <code class="highlighter-rouge">Ready</code> or another descriptive term. <code class="highlighter-rouge">juju status --color</code> will also return all green results when a cluster’s deployment is healthy.</p>

<p>Status can become unwieldy for large clusters, it is then recommended to check status on individual services, for example to check the status on the workers only:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju status kubernetes-worker
</code></pre></div></div>

<p>or just on the etcd cluster:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju status etcd
</code></pre></div></div>

<p>Errors will have an obvious message, and will return a red result when used with <code class="highlighter-rouge">juju status --color</code>. Nodes that come up in this manner should be investigated.</p>

<h2 id="sshing-to-units">SSHing to units</h2>

<p>You can ssh to individual units easily with the following convention, <code class="highlighter-rouge">juju ssh &lt;servicename&gt;/&lt;unit#&gt;</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju ssh kubernetes-worker/3
</code></pre></div></div>

<p>Will automatically ssh you to the 3rd worker unit.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju ssh easyrsa/0
</code></pre></div></div>

<p>This will automatically ssh you to the easyrsa unit.</p>

<h2 id="collecting-debug-information">Collecting debug information</h2>

<p>To collect comprehensive debug output from your Charmed Kubernetes cluster, install and run
<a href="https://github.com/juju/juju-crashdump">juju-crashdump</a> on a computer that has the Juju client installed, with the current controller and model pointing at your Charmed Kubernetes deployment.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>snap <span class="nb">install </span>juju-crashdump <span class="nt">--classic</span> <span class="nt">--channel</span> edge
juju-crashdump <span class="nt">-a</span> debug-layer <span class="nt">-a</span> config
</code></pre></div></div>

<p>Running the <code class="highlighter-rouge">juju-crashdump</code> script will generate a tarball of debug information that includes systemd unit status and logs, Juju logs, charm unit data, and Kubernetes cluster information. It is recommended that you include this tarball when <a href="https://bugs.launchpad.net/charmed-kubernetes">filing a bug</a>.</p>

<h2 id="common-problems">Common Problems</h2>

<h3 id="load-balancer-interfering-with-helm">Load Balancer interfering with Helm</h3>

<p>This section assumes you have a working deployment of Kubernetes via Juju using a Load Balancer for the API, and that you are using Helm to deploy charts.</p>

<p>To deploy Helm you will have run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm init
<span class="nv">$HELM_HOME</span> has been configured at /home/ubuntu/.helm
Tiller <span class="o">(</span>the helm server side component<span class="o">)</span> has been installed into your Kubernetes Cluster.
Happy Helming!
</code></pre></div></div>

<p>Then when using helm you may see one of the following errors:</p>

<ul>
  <li>Helm doesn’t get the version from the Tiller server</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm version
Client: &amp;version.Version<span class="o">{</span>SemVer:<span class="s2">"v2.1.3"</span>, GitCommit:<span class="s2">"5cbc48fb305ca4bf68c26eb8d2a7eb363227e973"</span>, GitTreeState:<span class="s2">"clean"</span><span class="o">}</span>
Error: cannot connect to Tiller
</code></pre></div></div>

<ul>
  <li>Helm cannot install your chart</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm <span class="nb">install</span> &lt;chart&gt; <span class="nt">--debug</span>
Error: forwarding ports: error upgrading connection: Upgrade request required
</code></pre></div></div>

<p>This is caused by the API load balancer not forwarding ports in the context of the helm client-server relationship. To deploy using helm, you will need to follow these steps:</p>

<ol>
  <li>
    <p>Expose the Kubernetes Master service</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju expose kubernetes-master
</code></pre></div>    </div>
  </li>
  <li>
    <p>Identify the public IP address of one of your masters</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> juju status kubernetes-master
</code></pre></div>    </div>

    <pre><code class="language-no-highlight">Model                         Controller          Cloud/Region   Version  SLA          Timestamp
conjure-canonical-kubern-ade  conjure-up-aws-91c  aws/eu-west-1  2.4.5    unsupported  08:39:23+01:00

App                Version  Status  Scale  Charm              Store       Rev  OS      Notes
flannel            0.10.0   active      2  flannel            jujucharms  146  ubuntu  
kubernetes-master  1.12.1   active      2  kubernetes-master  jujucharms  219  ubuntu  

Unit                  Workload  Agent  Machine  Public address  Ports     Message
kubernetes-master/0*  active    idle   6        34.254.175.71   6443/tcp  Kubernetes master running.
  flannel/0*          active    idle            34.254.175.71             Flannel subnet 10.1.16.1/24
kubernetes-master/1   active    idle   7        52.210.61.51    6443/tcp  Kubernetes master running.
  flannel/3           active    idle            52.210.61.51              Flannel subnet 10.1.38.1/24

Entity  Meter status  Message
model   amber         user verification pending  

Machine  State    DNS            Inst id              Series  AZ          Message
6        started  34.254.175.71  i-0f18d3f7377ba406f  bionic  eu-west-1a  running
7        started  52.210.61.51   i-08ec1daf25fb18fa3  bionic  eu-west-1b  running
</code></pre>

    <p>In this context the public IP address is 54.210.100.102.</p>

    <p>If you want to access this data programmatically you can use the JSON output:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju show-status kubernetes-master <span class="nt">--format</span> json | jq <span class="nt">--raw-output</span> <span class="s1">'.applications."kubernetes-master".units | keys[]'</span>
54.210.100.102
</code></pre></div>    </div>
  </li>
  <li>
    <p>Update the kubeconfig file</p>

    <p>Identify the kubeconfig file or section used for this cluster, and edit the server configuration.</p>

    <p>By default, it will look like <code class="highlighter-rouge">https://54.213.123.123:443</code>. Replace it with the Kubernetes Master endpoint <code class="highlighter-rouge">https://54.210.100.102:6443</code> and save.</p>

    <p>Note that the default port used by Charmed Kubernetes for the Kubernetes Master API is 6443 while the port exposed by the load balancer is 443.</p>
  </li>
  <li>
    <p>Start helm again!</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>helm install &lt;chart&gt; --debug
Created tunnel using local port: '36749'
SERVER: "localhost:36749"
CHART PATH: /home/ubuntu/.helm/&lt;chart&gt;
NAME:   &lt;chart&gt;
...
...
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="logging-and-monitoring">Logging and monitoring</h2>

<p>By default there is no log aggregation of the Kubernetes nodes, each node logs locally. Please read over the <a href="/kubernetes/docs/logging">logging</a> page for more information.</p>

<h2 id="troubleshooting-keystoneldap-issues">Troubleshooting Keystone/LDAP issues</h2>

<p>The following section offers some notes to help determine issues with using Keystone
for authentication/authorisation.</p>

<p>Testing the steps is important to determine the cause of the problem.</p>

<h3 id="can-you-communicate-with-keystone-and-get-an-authorization-token">Can you communicate with Keystone and get an authorization token?</h3>

<p>First is to verify that Keystone communication works from both your client and
the kubernetes-worker machines. The easiest thing to do here is to copy the
kube-keystone.sh script to the machines of interest from kubernetes-master with
<code class="highlighter-rouge">juju scp kubernetes-master/0:kube-keystone.sh .</code>, edit the script to include
your credentials, <code class="highlighter-rouge">source kube-keystone.sh</code> and then run <code class="highlighter-rouge">get_keystone_token</code>.
This will produce a token from the Keystone server. If that isn’t working,
check firewall settings on your Keystone server. Note that the
kube-keystone.sh script could be overwritten, so it is a best practice to make
a copy somewhere and use that.</p>

<h3 id="are-the-pods-for-keystone-authentication-up-and-running-properly">Are the pods for Keystone authentication up and running properly?</h3>

<p>The Keystone pods live in the kube-system namespace and read a configmap from
Kubernetes for the policy. Check to make sure they are running:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kube-system get po
</code></pre></div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                                              READY   STATUS    RESTARTS   AGE
k8s-keystone-auth-5c6b7f9b7c-mvvkx                1/1     Running   0          21m
k8s-keystone-auth-5c6b7f9b7c-q2jfq                1/1     Running   0          21m
</code></pre></div></div>

<p>Check the logs of the pods for errors:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kube-system logs k8s-keystone-auth-5c6b7f9b7c-mvvkx
</code></pre></div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W1121 05:02:02.878988       1 config.go:73] Argument <span class="nt">--sync-config-file</span> or <span class="nt">--sync-configmap-name</span> missing. Data synchronization between Keystone and Kubernetes is disabled.
I1121 05:02:02.879139       1 keystone.go:527] Creating kubernetes API client.
W1121 05:02:02.879151       1 client_config.go:548] Neither <span class="nt">--kubeconfig</span> nor <span class="nt">--master</span> was specified.  Using the inClusterConfig.  This might not work.
I1121 05:02:02.893499       1 keystone.go:544] Kubernetes API client created, server version v1.12
I1121 05:02:02.998944       1 keystone.go:93] ConfigMaps synced and ready
I1121 05:02:02.999045       1 keystone.go:101] Starting webhook server...
I1121 05:02:02.999262       1 keystone.go:155] ConfigMap created or updated, will update the authorization policy.
I1121 05:02:02.999459       1 keystone.go:171] Authorization policy updated.
</code></pre></div></div>

<h3 id="is-the-configmap-with-the-policy-correct">Is the configmap with the policy correct?</h3>

<p>Check the configmap contents. The pod logs above would complain if the YAML
isn’t valid, but make sure it matches what you expect.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl <span class="nt">-n</span> kube-system get configmap k8s-auth-policy <span class="nt">-o</span><span class="o">=</span>yaml
</code></pre></div></div>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">policies</span><span class="pi">:</span> <span class="pi">|</span>
    <span class="no">[</span>
      <span class="no">{</span>
        <span class="no">"resource": {</span>
          <span class="no">"verbs": ["get", "list", "watch"],</span>
          <span class="no">"resources": ["pods"],</span>
          <span class="no">"version": "*",</span>
          <span class="no">"namespace": "default"</span>
        <span class="no">},</span>
        <span class="no">"match": [</span>
          <span class="no">{</span>
            <span class="no">"type": "user",</span>
            <span class="no">"values": ["admin"]</span>
          <span class="no">},</span>
        <span class="no">]</span>
      <span class="no">}</span>
    <span class="no">]</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ConfigMap</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">kubectl.kubernetes.io/last-applied-configuration</span><span class="pi">:</span> <span class="pi">|</span>
      <span class="no">{"apiVersion":"v1","data":{"policies":"[\n  {\n    \"resource\": {\n      \"verbs\": [\"get\", \"list\", \"watch\"],\n      \"resources\": [\"pods\"],\n      \"version\": \"*\",\n      \"namespace\": \"default\"\n    },\n    \"match\": [\n      {\n        \"type\": \"user\",\n        \"values\": [\"admin\"]\n      },\n    ]\n  }\n]\n"},"kind":"ConfigMap","metadata":{"annotations":{},"labels":{"k8s-app":"k8s-keystone-auth"},"name":"k8s-auth-policy","namespace":"kube-system"}}</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="s">2018-11-21T02:38:12Z</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">k8s-app</span><span class="pi">:</span> <span class="s">k8s-keystone-auth</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">k8s-auth-policy</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">resourceVersion</span><span class="pi">:</span> <span class="s2">"</span><span class="s">16736"</span>
  <span class="na">selfLink</span><span class="pi">:</span> <span class="s">/api/v1/namespaces/kube-system/configmaps/k8s-auth-policy</span>
  <span class="na">uid</span><span class="pi">:</span> <span class="s">7dc0842b-ed36-11e8-82e1-06d4a9ac9e06</span>
</code></pre></div></div>

<h3 id="check-the-service-and-endpoints">Check the service and endpoints</h3>

<p>Verify the service exists and has endpoints</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get svc <span class="nt">-n</span> kube-system
</code></pre></div></div>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>             AGE
heapster                    ClusterIP   10.152.183.49    &lt;none&gt;        80/TCP              136m
<span class="k">**</span>k8s-keystone-auth-service   ClusterIP   10.152.183.200   &lt;none&gt;        8443/TCP            105m<span class="k">**</span>
kube-dns                    ClusterIP   10.152.183.218   &lt;none&gt;        53/UDP,53/TCP       137m
kubernetes-dashboard        ClusterIP   10.152.183.142   &lt;none&gt;        443/TCP             136m
metrics-server              ClusterIP   10.152.183.245   &lt;none&gt;        443/TCP             136m
monitoring-grafana          ClusterIP   10.152.183.2     &lt;none&gt;        80/TCP              136m
monitoring-influxdb         ClusterIP   10.152.183.172   &lt;none&gt;        8083/TCP,8086/TCP   136m
<span class="nv">$ </span>kubectl <span class="nt">-n</span> kube-system get ep
NAME                        ENDPOINTS                       AGE
heapster                    10.1.20.4:8082                  136m
<span class="k">**</span>k8s-keystone-auth-service   10.1.20.5:8443,10.1.32.4:8443   105m<span class="k">**</span>
kube-controller-manager     &lt;none&gt;                          136m
kube-dns                    10.1.31.6:53,10.1.31.6:53       136m
kube-scheduler              &lt;none&gt;                          136m
kubernetes-dashboard        10.1.31.3:8443                  136m
metrics-server              10.1.32.2:443                   136m
monitoring-grafana          10.1.31.2:3000                  136m
monitoring-influxdb         10.1.31.2:8086,10.1.31.2:8083   136m
</code></pre></div></div>

<h3 id="attempt-to-authenticate-directly-to-the-service">Attempt to authenticate directly to the service</h3>

<p>Use a token to auth with the Keystone service directly:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> | curl -ks -XPOST -d @- https://10.152.183.200:8443/webhook | python -mjson.tool
{
  "apiVersion": "authentication.k8s.io/v1beta1",
  "kind": "TokenReview",
  "metadata": {
    "creationTimestamp": null
  },
  "spec": {
    "token": "</span><span class="k">$(</span>get_keystone_token<span class="k">)</span><span class="sh">"
  }
}
</span><span class="no">EOF

</span><span class="o">{</span>
    <span class="s2">"apiVersion"</span>: <span class="s2">"authentication.k8s.io/v1beta1"</span>,
    <span class="s2">"kind"</span>: <span class="s2">"TokenReview"</span>,
    <span class="s2">"metadata"</span>: <span class="o">{</span>
        <span class="s2">"creationTimestamp"</span>: null
    <span class="o">}</span>,
    <span class="s2">"spec"</span>: <span class="o">{</span>
        <span class="s2">"token"</span>: <span class="s2">"gAAAAABb9Yeel_62KoSb_fAL6RPMpGZ4-4y5RLqXq5YdY3PcIKpuIcZ8PoVPhQtHOR7fiPYpFQX_pAUZJ4yngSE_WbJeuX8c-pl5WgStNImmkH3sEvQ5nSfimGhQSH-k5ydCBhcor87AeN7dOS-X6zHMRrcyvnZffQ"</span>
    <span class="o">}</span>,
    <span class="s2">"status"</span>: <span class="o">{</span>
        <span class="s2">"authenticated"</span>: <span class="nb">true</span>,
        <span class="s2">"user"</span>: <span class="o">{</span>
            <span class="s2">"extra"</span>: <span class="o">{</span>
                <span class="s2">"alpha.kubernetes.io/identity/project/id"</span>: <span class="o">[</span>
                    <span class="s2">""</span>
                <span class="o">]</span>,
                <span class="s2">"alpha.kubernetes.io/identity/project/name"</span>: <span class="o">[</span>
                    <span class="s2">""</span>
                <span class="o">]</span>,
                <span class="s2">"alpha.kubernetes.io/identity/roles"</span>: <span class="o">[]</span>,
                <span class="s2">"alpha.kubernetes.io/identity/user/domain/id"</span>: <span class="o">[</span>
                    <span class="s2">"e1cbddf1b75340499109f0b88b28d472"</span>
                <span class="o">]</span>,
                <span class="s2">"alpha.kubernetes.io/identity/user/domain/name"</span>: <span class="o">[</span>
                    <span class="s2">"admin_domain"</span>
                <span class="o">]</span>
            <span class="o">}</span>,
            <span class="s2">"groups"</span>: <span class="o">[</span>
                <span class="s2">""</span>
            <span class="o">]</span>,
            <span class="s2">"uid"</span>: <span class="s2">"432f311e7eb94689b10aee03293ab030"</span>,
            <span class="s2">"username"</span>: <span class="s2">"admin"</span>
        <span class="o">}</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>Note that you need to change the IP address above to the address of your
<code class="highlighter-rouge">k8s-keystone-auth-service</code>. This will talk to the webhook and verify that the token is
valid and return information about the user.</p>

<h3 id="api-server">API server</h3>

<p>Finally, communication between the API server and the Keystone service is verified. The
easiest thing to do here is to look at the log for the API server for interesting information
such as timeouts or errors with the webhook.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>juju run <span class="nt">--unit</span> kubernetes-master/0 <span class="nt">--</span> journalctl <span class="nt">-u</span> snap.kube-apiserver.daemon.service
</code></pre></div></div>

<!-- FEEDBACK -->
<div class="p-notification--information">
  <p class="p-notification__response">
    We appreciate your feedback on the documentation. You can 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/edit/master/pages/k8s/troubleshooting.md" class="p-notification__action">edit this page</a> 
    or 
    <a href="https://github.com/charmed-kubernetes/kubernetes-docs/issues/new" class="p-notification__action">file a bug here</a>.
  </p>
</div>
